import csv
import os
import shutil
import nike_scraper
import google_drive

# CMPT 370 Fall 2020
# This program groups data from csv files generated by individual scrapers
# into 10 files. For each of men and women, 5 files are generated of the
# categories tops, bottoms, overall, footwear, and accessories.


def get_categories():
    """
    Stores overall categories
    :return: (list) names of categories as strings
    """
    categories = ["tops", "bottoms", "overall", "footwear", "accessories"]
    return categories


def get_scraper_files(category, gender):
    """
    Retrieves names of all scraper output csv files related to the
    specified category
    :param: category: (string) name of category of interest
    :return: csv_files: (list) strings representing file names
             first item is the list's identifier (i.e. men_tops)
    """
    csv_files = []
    # Use line below to specify cache data location
    current_files = os.listdir("scraper_data/")

    identifier = category + "_" + gender
    csv_files.append(identifier)

    # extract csv files
    for file in current_files:
        if file.endswith(category + "_" + gender + ".csv"):
            csv_files.append(file)

    return csv_files


def group_by_gender(categories):
    """
    Takes a list of categories and groups related csv files
    according to gender.
    :param: categories: (list) overall clothing categories (e.g. tops)
    :return: tuple of mens_files and womens_files
    """
    mens_files = []
    womens_files = []
    for category in categories:
        mens_files.append(get_scraper_files(category, "men"))
        womens_files.append(get_scraper_files(category, "women"))
    return mens_files, womens_files


def combine_files(list_of_files):
    """
    Creates a csv file that combined results of the same category from scrapers
    :param: (list) names of files to be combined
    :return: (string) filename of csv created
    """
    new_file_name = list_of_files[0] + "_cache.csv"

    # Add header
    fields = ["Name", "Gender", "Price", "Sale Price", "Colors",
              "Item Link", "Image Link", "Subcategory", "Store"]
    if not os.path.exists("cache_storage/"):
        os.mkdir("cache_storage/")
    f = csv.writer(open("cache_storage/" + new_file_name, "w", newline=''))
    f.writerow(fields)

    for filename in list_of_files[1:]:
        reader = csv.reader(open("scraper_data/" + filename))

        with open("cache_storage/" + new_file_name, "a", newline="") as f:
            writer = csv.writer(f)

            # Exclude headers within individual files
            first = True
            for r in reader:
                if not first:
                    writer.writerow(r)
                else:
                    first = False


def backup_cache():
    """
    Transfers previous cache files to a backup subdirectory in case of
    failure when generating new cache data.
    NOTE: Future development: store a week's worth of data to allow for
    trend analysis.
    """
    # Only if scrapers are functioning correctly, update cache
    src_dir = "cache_storage/"
    dst_dir = "backup_cache_storage/"
    if not os.path.exists("backup_cache_storage/"):
        os.mkdir("backup_cache_storage/")
    for root, dirs, files in os.walk(src_dir):
        for f in files:
            if f.endswith('.csv'):
                shutil.copy(os.path.join(root, f), dst_dir)


def restore_cache():
    """
    Transfers previous cache files from backup.
    """
    # Only if scrapers are functioning correctly, update cache
    src_dir = "backup_cache_storage/"
    dst_dir = "cache_storage/"
    for root, dirs, files in os.walk(src_dir):
        for f in files:
            if f.endswith('.csv'):
                shutil.copy(os.path.join(root, f), dst_dir)


def run_scrapers():
    """
    Run each of the scrapers and generate all .csv files.
    :return: None
    """
    nike_scraper.main()


def move_files():
    """
    Move all .csv files to the scraper_data directory.
    :return: None
    """
    # Only if scrapers are functioning correctly, update cache
    src_dir = "."
    dst_dir = "scraper_data/"
    if not os.path.exists("scraper_data/"):
        os.mkdir("scraper_data/")
    for root, dirs, files in os.walk(src_dir):
        for f in files:
            if f.endswith('.csv'):
                if f != "results.csv":
                    shutil.move(os.path.join(root, f), os.path.join(dst_dir, f))


def main():
    # assumption for now that all scraper output is in the
    # same directory as this file

    backup_cache()
    run_scrapers()
    move_files()

    categories = get_categories()
    data_by_gender = group_by_gender(categories)
    # Retrieve mens data
    for group in data_by_gender[0]:
        combine_files(group)
    # Retrieve womens data
    for group in data_by_gender[1]:
        combine_files(group)

    google_drive.main()


# main
if __name__ == '__main__':
    main()
